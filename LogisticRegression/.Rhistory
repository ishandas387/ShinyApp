# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
# Summarize the model
summary(model)
# Model accuracy
mean(predicted.classes == test.data$diabetes)
data("PimaIndiansDiabetes2", package = "mlbench")
#This just omits the empty values. Just 1 part of data analysis/creation
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
# Split the data into training and test set
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
# Fit the model
model <- glm( diabetes ~., data = train.data, family = binomial)
# Summarize the model
summary(model)
"Estimate: the intercept (b0) and the beta coefficient estimates associated to each predictor variable
Std.Error: the standard error of the coefficient estimates. This represents the accuracy of the coefficients.
<b>The larger the standard error, the less confident we are about the estimate.<b>
z value: the z-statistic, which is the coefficient estimate (column 2) divided by the standard error of the estimate (column 3)
Pr(>|z|): The p-value corresponding to the z-statistic. $$ The smaller the p-value, the more significant the estimate is. $$"
# Make predictions
probabilities <- model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Model accuracy
mean(predicted.classes == test.data$diabetes)
summary(modelSingle)
#using a random sample data, not the test set.
newdata <- data.frame(glucose = c(20,  180))
View(newdata)
probabilities <- modelSingle %>% predict(newdata, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
predicted.classes
#using a random sample data, not the test set.
newdata <- data.frame(glucose = c(20,  180))
probabilitiesgl <- modelSingle %>% predict(newdata, type = "response")
predicted.classesgl <- ifelse(probabilitiesgl > 0.5, "pos", "neg")
predicted.classesgl
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp/ClassificationApp/Classification.R')
summary(modelSingle)
library(e1071)
dataset=iris
acc=c(0,0,0);
mc=1000
mc=100
for(i in 1:mc){
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
#build model � linear kernel and C-classification (soft margin) with default cost (C=1)
svm_model <- svm(Species~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$Species
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
# NB
nb_model <- naiveBayes(Species~ ., data=trainset)
pred_nb= predict(nb_model, testset)
accuracy_nb=mean(pred_nb==actual)
# Multinomial Logistic reg
library(nnet)
mlr_model <- multinom(Species~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
accuracy_mlr=mean(pred_mlr==actual)
vect_accuracy=c(accuracy_svm,accuracy_nb,accuracy_mlr)
acc=acc+(1/mc)*vect_accuracy
}
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(Species~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$Species
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
nb_model <- naiveBayes(Species~ ., data=trainset)
pred_nb= predict(nb_model, testset)
accuracy_nb=mean(pred_nb==actual)
library(nnet)
mlr_model <- multinom(Species~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
accuracy_mlr=mean(pred_mlr==actual)
vect_accuracy=c(accuracy_svm,accuracy_nb,accuracy_mlr)
acc=acc+(1/mc)*vect_accuracy
for(i in 1:mc){
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
#build model � linear kernel and C-classification (soft margin) with default cost (C=1)
svm_model <- svm(Species~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$Species
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
# NB
nb_model <- naiveBayes(Species~ ., data=trainset)
pred_nb= predict(nb_model, testset)
accuracy_nb=mean(pred_nb==actual)
# Multinomial Logistic reg
library(nnet)
mlr_model <- multinom(Species~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
accuracy_mlr=mean(pred_mlr==actual)
vect_accuracy=c(accuracy_svm,accuracy_nb,accuracy_mlr)
acc=acc+(1/mc)*vect_accuracy
}
acc
acc
dataset=Seatbelts
dataset
#load built-in iris dataset
Seatbelts
#load built-in iris dataset
data(Seatbelts)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(law~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$law
pred_svm= predict(svm_model, testset, type='response')
svm_model <- svm(law~ ., data=trainset, method="C-classification", kernel="linear")
library(e1071)
dataset=Seatbelts
acc=c(0,0,0);
mc=1
for(i in 1:mc){
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
#build model � linear kernel and C-classification (soft margin) with default cost (C=1)
svm_model <- svm(Species~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$Species
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
# NB
nb_model <- naiveBayes(Species~ ., data=trainset)
pred_nb= predict(nb_model, testset)
accuracy_nb=mean(pred_nb==actual)
# Multinomial Logistic reg
library(nnet)
mlr_model <- multinom(Species~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
accuracy_mlr=mean(pred_mlr==actual)
vect_accuracy=c(accuracy_svm,accuracy_nb,accuracy_mlr)
acc=acc+(1/mc)*vect_accuracy
}
library(e1071)
dataset=Seatbelts
acc=c(0,0,0);
mc=1
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(law~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$law
svm_model <- svm(law~ ., data=trainset, method="C-classification", kernel="linear")
View(svm_model)
actual=testset$law
dataset=data.frame(Seatbelts)
acc=c(0,0,0);
mc=1
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(law~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$law
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
pred_svm
library(e1071)
library(nnet)
dataset=data.frame(Seatbelts)
acc=c(0,0,0);
mc=1
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
View(svm_model)
View(dataset)
table(dataset$law)
dataset=data.frame(mtcars)
acc=c(0,0,0);
mc=1
View(dataset)
table(dataset$vs)
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(vs~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$vs
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
pred_svm
#getting a sense of data of what we are predicting.
#if the data set has balanced stuff.
table(df$admit)
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
str(df)
summary(df)
xtabs(~ admit +rank ,data=df)
#getting a sense of data of what we are predicting.
#if the data set has balanced stuff.
table(df$admit)
#load built-in iris dataset
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
dataset=df
acc=c(0,0,0);
mc=1
table(dataset$admit)
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(admit~ ., data=trainset, method="C-classification", kernel="linear")
actual=testset$admit
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
View(dataset)
View(dataset)
library(e1071)
library(nnet)
#load built-in iris dataset
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
dataset=df
acc=c(0,0,0);
mc=1
table(dataset$admit)
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(admit~ ., data=trainset, method="nu-classification", kernel="linear")
actual=testset$admit
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
pred_svm
nb_model <- naiveBayes(admit~ ., data=trainset)
pred_nb= predict(nb_model, testset)
accuracy_nb=mean(pred_nb==actual)
accuracy_nb
nb_model <- naiveBayes(admit~ ., data=trainset)
pred_nb= predict(nb_model, testset)
mlr_model <- multinom(admit~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
accuracy_mlr=mean(pred_mlr==actual)
accuracy_mlr
nb_model <- naiveBayes(admit~ ., data=trainset)
pred_nb= predict(nb_model, testset)
accuracy_nb=mean(pred_nb==actual)
accuracy_nb
pred_mlr
pred_nb
pred_nb
data("PimaIndiansDiabetes2", package = "mlbench")
#This just omits the empty values. Just 1 part of data analysis/creation
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
dataset=PimaIndiansDiabetes2
table(dataset$diabetes)
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
svm_model <- svm(diabetes~ ., data=trainset, method="nu-classification", kernel="linear")
actual=testset$diabetes
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
pred_svm
nb_model <- naiveBayes(diabetes~ ., data=trainset)
pred_nb= predict(nb_model, testset)
pred_nb
qaccuracy_nb=mean(pred_nb==actual)
accuracy_nb
mlr_model <- multinom(diabetes~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
pred_mlr
accuracy_mlr=mean(pred_mlr==actual)
accuracy_mlr
confusionMatrix(pred_nb,actual)
accuracy_nb=mean(pred_nb==actual)
accuracy_nb
#split into training and test sets
for(i in 1:mc){
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
#build model linear kernel and C-classification (soft margin) with default cost (C=1)
svm_model <- svm(diabetes~ ., data=trainset, method="nu-classification", kernel="linear")
actual=testset$diabetes
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
pred_svm
# NB
nb_model <- naiveBayes(diabetes~ ., data=trainset)
pred_nb= predict(nb_model, testset)
pred_nb
confusionMatrix(pred_nb,actual)
accuracy_nb=mean(pred_nb==actual)
accuracy_nb
# Multinomial Logistic reg
mlr_model <- multinom(diabetes~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
pred_mlr
accuracy_mlr=mean(pred_mlr==actual)
accuracy_mlr
vect_accuracy=c(accuracy_svm,accuracy_nb,accuracy_mlr)
acc=acc+(1/mc)*vect_accuracy
}
mc=100
#load required library
library(e1071)
library(nnet)
data("PimaIndiansDiabetes2", package = "mlbench")
"Data cleanup should include the following-
Remove potential outliers
Make sure that the predictor variables are normally distributed.
If not, you can use log, root, Box-Cox transformation.
Remove highly correlated predictors to minimize overfitting.
The presence of highly correlated predictors might lead to an unstable model
solution."
#This just omits the empty values. Just 1 part of data analysis/creation
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
# Inspect the data
sample_n(PimaIndiansDiabetes2, 3)
#load built-in iris dataset
df <- read.csv("https://stats.idre.ucla.edu/stat/data/binary.csv")
dataset=PimaIndiansDiabetes2
acc=c(0,0,0);
mc=100
table(dataset$diabetes)
#split into training and test sets
for(i in 1:mc){
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
#build model linear kernel and C-classification (soft margin) with default cost (C=1)
svm_model <- svm(diabetes~ ., data=trainset, method="nu-classification", kernel="linear")
actual=testset$diabetes
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
pred_svm
# NB
nb_model <- naiveBayes(diabetes~ ., data=trainset)
pred_nb= predict(nb_model, testset)
pred_nb
confusionMatrix(pred_nb,actual)
accuracy_nb=mean(pred_nb==actual)
accuracy_nb
# Multinomial Logistic reg
mlr_model <- multinom(diabetes~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
pred_mlr
accuracy_mlr=mean(pred_mlr==actual)
accuracy_mlr
vect_accuracy=c(accuracy_svm,accuracy_nb,accuracy_mlr)
acc=acc+(1/mc)*vect_accuracy
}
acc
ggplot(acc, aes(x=dose, y=len, fill=dose)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(data.frame(acc), aes(x=dose, y=len, fill=dose)) +
geom_bar(stat="identity")+theme_minimal()
acc
as.data.frame(acc)
acc=c(SVM =0,NB= 0, MLR =0);
mc=100
table(dataset$diabetes)
#split into training and test sets
for(i in 1:mc){
n=nrow(dataset)
indexes = sample(n,n*(80/100))
trainset = dataset[indexes,]
testset = dataset[-indexes,]
#build model linear kernel and C-classification (soft margin) with default cost (C=1)
svm_model <- svm(diabetes~ ., data=trainset, method="nu-classification", kernel="linear")
actual=testset$diabetes
pred_svm= predict(svm_model, testset, type='response')
accuracy_svm=mean(pred_svm==actual)
accuracy_svm
pred_svm
# NB
nb_model <- naiveBayes(diabetes~ ., data=trainset)
pred_nb= predict(nb_model, testset)
pred_nb
confusionMatrix(pred_nb,actual)
accuracy_nb=mean(pred_nb==actual)
accuracy_nb
# Multinomial Logistic reg
mlr_model <- multinom(diabetes~ ., data=trainset)
pred_mlr= predict(mlr_model, testset)
pred_mlr
accuracy_mlr=mean(pred_mlr==actual)
accuracy_mlr
vect_accuracy=c(accuracy_svm,accuracy_nb,accuracy_mlr)
acc=acc+(1/mc)*vect_accuracy
}
as.data.frame(acc)
ggplot(data.frame(acc), aes(x=dose, y=accuracy, fill=dose)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(data.frame(acc), aes(x=, y=accuracy, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(data.frame(acc), aes(x=, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(data.frame(acc), aes(x=a, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(data.frame(acc), aes(x=a, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
as.data.frame(acc)
plotdf <- as.data.frame(acc)
ggplot(plotdf, aes(x=a, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(plotdf, aes(x=acc, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(plotdf, aes(x="", y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(plotdf, aes(x=SVM, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(plotdf, aes(x=acc, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
ggplot(plotdf, aes(x=acc, y=acc, fill="")) +
geom_bar(stat="identity")+theme_minimal()
ggplot(plotdf, aes(x=acc, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
acc
ggplot(acc, aes(x=acc, y=acc, fill=acc)) +
geom_bar(stat="identity")+theme_minimal()
plotdf <- as.data.frame(acc)
plotdf
acc
acc
plotdf
plot(plotdf, M[,1],ylim=c(0.93,.98),col='blue', type='b')
plot(plotdf, acc,ylim=c(0.93,.98),col='blue', type='b')
#part A
#load the dataset
link='http://users.stat.ufl.edu/~winner/data/nfl2008_fga.csv'
data=read.csv(link)
#togo, ydline, kicker
y=data$homekick
x1=data$togo
x2=data$ydline
x3=data$kicker
df=data.frame(x1,x2,x3,y)
#part B
#data cleansing: removing missing values, removing outliers......
dataset1 <- na.omit(df)#removing outliers
set.seed(167)
####split dataset
n=nrow(dataset1)
indexes = sample(n,n*(80/100))
trainset = dataset1[indexes,]
testset = dataset1[-indexes,]
full.model= glm(trainset$y~., data= trainset, family='binomial') #Fitting the lineaer reg Model
summary(full.model)
#part C
phat_i=predict(full.model,testset, type="response")  # phat_i
predictedvalues=rep(0,length(phat_i))
predictedvalues[phat_i>0.5]=1   # probability of gender being 1, if p<0.5 then gender=0
actual=testset$y
df=data.frame(actual,predictedvalues)
#part D
confusion_matrix=table( predictedvalues, actualvalues=actual) #confusion matrix
accuracy=mean(predictedvalues == actual) # accuary
accuracy
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
runApp('C:/Users/ishan/Downloads/CA/ShinyApp')
